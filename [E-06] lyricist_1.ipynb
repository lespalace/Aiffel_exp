{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 작사가 인공지능 만들기\n",
    "\n",
    "#### 목표 \n",
    "- 텍스트 제너레이션 결과 그럴듯한 문장으로 생성하기\n",
    "- 특수문자 제거, 토크나이저 생성, 패딩처리 등 전처리 과정\n",
    "- 텍스트 생성모델의 validation loss 값 2.2 수준으로 달성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 라이브러리 import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#효율적인 interator를 위한 툴 모듈\n",
    "import itertools \n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained']\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 txt 파일을 모두 raw_corpus 에 담기 \n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel//lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel//lyricist/data/lyrics/kanye-west.txt /aiffel/aiffel//lyricist/data/lyrics/Kanye_West.txt\n",
      "/aiffel/aiffel//lyricist/data/lyrics/notorious-big.txt /aiffel/aiffel//lyricist/data/lyrics/notorious_big.txt\n"
     ]
    }
   ],
   "source": [
    "# 중복된 문장들이 많은 파일들은 앞서 제거하는 함수\n",
    "\n",
    "def Duplicate_f(file1, file2):\n",
    "    txt_before = []\n",
    "    txt_after = []\n",
    "    with open(file1, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        txt_before.extend(raw)\n",
    "    with open(file2, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        txt_after.extend(raw)\n",
    "    txt_before = set(txt_before)\n",
    "    txt_after = set(txt_after)\n",
    "    diff = txt_before.difference(txt_after)\n",
    "    return len(txt_before) * 0.05 > len(diff)\n",
    "\n",
    "# 반복하면서 체크하고 해당되는 파일들을 리스트에서 제거\n",
    "for a, b in itertools.combinations(txt_list, 2):\n",
    "    if Duplicate_f(a, b):\n",
    "        print(a, b)\n",
    "        txt_list.remove(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Busted flat in Baton Rouge, waitin' for a train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And I's feelin' near as faded as my jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bobby thumbed a diesel down, just before it ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It rode us all the way to New Orleans I pulled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was playin' soft while Bobby sang the blues,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187083</th>\n",
       "      <td>1 2 3 4. Fiasco, fiasco [Chorus:]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187084</th>\n",
       "      <td>1 2 3 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187085</th>\n",
       "      <td>I'm calling, I'm calling, I'm calling it fiasco,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187086</th>\n",
       "      <td>I'm calling, I'm calling, I'm calling it fiasco,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187087</th>\n",
       "      <td>I'm calling, I'm calling, I'm calling it fiasco.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187088 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0         Busted flat in Baton Rouge, waitin' for a train\n",
       "1               And I's feelin' near as faded as my jeans\n",
       "2       Bobby thumbed a diesel down, just before it ra...\n",
       "3       It rode us all the way to New Orleans I pulled...\n",
       "4       I was playin' soft while Bobby sang the blues,...\n",
       "...                                                   ...\n",
       "187083                  1 2 3 4. Fiasco, fiasco [Chorus:]\n",
       "187084                                            1 2 3 4\n",
       "187085   I'm calling, I'm calling, I'm calling it fiasco,\n",
       "187086   I'm calling, I'm calling, I'm calling it fiasco,\n",
       "187087  I'm calling, I'm calling, I'm calling it fiasco. \n",
       "\n",
       "[187088 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(raw_corpus)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busted flat in Baton Rouge, waitin' for a train\n",
      "And I's feelin' near as faded as my jeans\n",
      "Bobby thumbed a diesel down, just before it rained\n",
      "It rode us all the way to New Orleans I pulled my harpoon out of my dirty red bandanna\n",
      "I was playin' soft while Bobby sang the blues, yeah\n",
      "Windshield wipers slappin' time, I was holdin' Bobby's hand in mine\n",
      "We sang every song that driver knew Freedom's just another word for nothin' left to lose\n",
      "Nothin', don't mean nothin' hon' if it ain't free, no no\n",
      "And, feelin' good was easy, Lord, when he sang the blues\n",
      "You know, feelin' good was good enough for me\n"
     ]
    }
   ],
   "source": [
    "# 제대로 추려졌는지 문장들을 출력해서 확인\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue    # 길이가 0인 문장은 건너뜀\n",
    "    if sentence[-1] == \":\": continue   # 문장의 끝이 : 인 문장은 건너뜀\n",
    "\n",
    "    if idx > 9: break   # 문장 10개 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 데이터 정제\n",
    "\n",
    "- Preprocess_sentence() 함수를 활용해 데이터를 정제\n",
    "- 추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거\n",
    "- 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가면 잘라내기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식을 이용해서 문장들을 정리(특수문자 제거에 유의)\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1 소문자로 바꾸고, 양쪽 공백 삭제\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2 특수문자 양쪽에 공백 삽입\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3 여러개의 공백은 하나의 공백으로 교체\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4 a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 통일\n",
    "    sentence = sentence.strip() # 5 다시 양쪽 공백 삭제\n",
    "    sentence1 = sentence.split(' ')\n",
    "\n",
    "    # 갯수가 14개 이하의 문장들만을 추려낸다\n",
    "    if len(sentence1) > 13: \n",
    "        return 0\n",
    "    else:\n",
    "        sentence = '<start> ' + sentence + ' <end>' # 6 문장 시작에는 <start>, 끝에는 <end>를 추가\n",
    "        return sentence\n",
    "    \n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> busted flat in baton rouge , waitin for a train <end>',\n",
       " '<start> and i s feelin near as faded as my jeans <end>',\n",
       " '<start> bobby thumbed a diesel down , just before it rained <end>',\n",
       " '<start> i was playin soft while bobby sang the blues , yeah <end>',\n",
       " '<start> windshield wipers slappin time , i was holdin bobby s hand in mine <end>',\n",
       " '<start> you know , feelin good was good enough for me <end>',\n",
       " '<start> there bobby shared the secrets of my soul <end>',\n",
       " '<start> through all kinds of weather , through everything we done <end>',\n",
       " '<start> he s lookin for that home , and i hope he finds it <end>',\n",
       " '<start> nothin , that s all that bobby left me , yeah <end>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 앞서 정제된 문장들 중에서 필요한 문장들만 다시 리스트로 추려내기\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus: # 반복하면서 원하지 않는 문장은 패스\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if preprocessed_sentence ==0:\n",
    "        pass\n",
    "    else:\n",
    "        corpus.append(preprocess_sentence(sentence))\n",
    "                \n",
    "# 졍제된 문장을 10개 정도만 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 평가 데이터셋 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 4009 1674 ...    0    0    0]\n",
      " [   2    8    4 ...    0    0    0]\n",
      " [   2  871 6826 ...    0    0    0]\n",
      " ...\n",
      " [   2   22   22 ...    3    0    0]\n",
      " [   2   22 4254 ...    0    0    0]\n",
      " [   2    3    0 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7feab21cc790>\n"
     ]
    }
   ],
   "source": [
    "# 단어를 토큰화 시키기 \n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=17000,  # 전체 단어의 개수 \n",
    "        filters=' ',    \n",
    "        oov_token=\"<unk>\"  \n",
    "    )\n",
    "    \n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됨\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공\n",
    "    # maxlen의 디폴트값은 None - 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰짐\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "#     tensor = np.delete(tensor, slice(13, -1), axis=1)\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156013, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor 로 변환된 후의 크기를 확인\n",
    "\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2  4009  1674    14 12440  4486     5  1295    28     9]\n",
      " [    2     8     4    17   560   857    81  2711    81    13]\n",
      " [    2   871  6826     9  6263    58     5    32   174    11]\n",
      " [    2     4    53   998   924   230   871  2634     6  1119]\n",
      " [    2  5374  6827  4011    76     5     4    53  1141   871]\n",
      " [    2     7    34     5   560   115    53   115   255    28]\n",
      " [    2    62   871  3397     6  1730    20    13   300     3]\n",
      " [    2   128    25  3170    20  1675     5   128   177    21]\n",
      " [    2    54    17   457    28    15   159     5     8     4]\n",
      " [    2   497     5    15    17    25    15   871   243    12]]\n"
     ]
    }
   ],
   "source": [
    "# 제대로 바뀌었는지 확인을 위해 tensor 일부를 체크\n",
    "\n",
    "print(tensor[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25663"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인 \n",
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'chorus' in tokenizer.index_word.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : that\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 15: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2  4009  1674    14 12440  4486     5  1295    28     9   636     3\n",
      "     0     0]\n",
      "[ 4009  1674    14 12440  4486     5  1295    28     9   636     3     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "# 여기서 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]\n",
    "  \n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 평가 데이터셋 분리\n",
    "\n",
    "- testset 과 trainset 을 80% : 20% 으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train dataset: (124810, 14)\n",
      "- Train label: (124810, 14)\n",
      "- Validation dataset: (31203, 14)\n",
      "- Validation label: (31203, 14)\n"
     ]
    }
   ],
   "source": [
    "# 분리된 데이터 체크\n",
    "print(\"- Train dataset:\", enc_train.shape)\n",
    "print(\"- Train label:\", dec_train.shape)\n",
    "print(\"- Validation dataset:\", enc_val.shape)\n",
    "print(\"- Validation label:\", dec_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분리가 된 것을 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   14   15 1723    5   44 2044    5   43   57    5    7    1    3]\n",
      "[  14   15 1723    5   44 2044    5   43   57    5    7    1    3    0]\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[0])\n",
    "print(dec_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 14), (128, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 14), (128, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 모델 만들기 \n",
    "\n",
    "- embedding size, hidden size 를 조절\n",
    "- Epoch 10 이하 \n",
    "- validation loss 값을 2.2 이하로 달성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.batch_norm_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.batch_norm_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.batch_norm_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 조절 ----------------\n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "# ---------------------\n",
    "\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 14, 17001), dtype=float32, numpy=\n",
       "array([[[ 1.58492549e-04, -7.44797580e-05,  1.17044976e-04, ...,\n",
       "          3.48531059e-04, -2.53725913e-04,  4.36148839e-05],\n",
       "        [ 2.27430268e-04, -7.25518985e-05,  1.17454372e-04, ...,\n",
       "          6.51283888e-04, -3.27402755e-04,  3.24467578e-06],\n",
       "        [ 1.33342517e-04,  4.26046172e-05, -1.00617151e-04, ...,\n",
       "          1.03721407e-03, -3.48332425e-04, -3.01056454e-04],\n",
       "        ...,\n",
       "        [-1.42928737e-03,  3.13553697e-04, -2.97523383e-03, ...,\n",
       "          2.34041177e-03,  4.76527202e-04, -2.56053009e-03],\n",
       "        [-1.53309223e-03,  2.86865281e-04, -3.14585562e-03, ...,\n",
       "          2.32706964e-03,  5.96778176e-04, -2.65947590e-03],\n",
       "        [-1.62325322e-03,  2.62316258e-04, -3.28620337e-03, ...,\n",
       "          2.30668555e-03,  7.10810884e-04, -2.74861418e-03]],\n",
       "\n",
       "       [[ 1.58492549e-04, -7.44797580e-05,  1.17044976e-04, ...,\n",
       "          3.48531059e-04, -2.53725913e-04,  4.36148839e-05],\n",
       "        [ 5.67907642e-04, -3.33910604e-04,  2.92662909e-04, ...,\n",
       "          2.46996584e-04, -3.00143525e-04,  1.16947456e-04],\n",
       "        [ 8.70991091e-04, -3.37166071e-04,  8.93661563e-05, ...,\n",
       "          2.02407813e-04, -4.11193294e-04,  3.12948076e-04],\n",
       "        ...,\n",
       "        [ 1.10793673e-03,  3.75092000e-04,  7.79769849e-04, ...,\n",
       "          3.01691878e-04,  1.87526195e-04, -5.14234198e-05],\n",
       "        [ 1.12208631e-03,  4.92031861e-04,  8.39862740e-04, ...,\n",
       "          2.24163101e-04,  4.30189015e-04, -2.29217449e-06],\n",
       "        [ 1.10252795e-03,  4.73698630e-04,  7.85587181e-04, ...,\n",
       "          1.25486244e-04,  7.41247379e-04, -1.37454685e-04]],\n",
       "\n",
       "       [[ 1.58492549e-04, -7.44797580e-05,  1.17044976e-04, ...,\n",
       "          3.48531059e-04, -2.53725913e-04,  4.36148839e-05],\n",
       "        [ 1.07593609e-04, -2.79087399e-04,  8.39712302e-05, ...,\n",
       "          6.23758940e-04, -2.01227303e-04, -4.31378066e-06],\n",
       "        [ 9.80299083e-05, -2.64157541e-04,  4.48626743e-05, ...,\n",
       "          7.08328036e-04, -5.59443433e-04, -2.98574509e-04],\n",
       "        ...,\n",
       "        [-1.19530980e-03,  6.29410904e-04, -1.75934122e-03, ...,\n",
       "          1.61222392e-03, -1.11876393e-03, -2.00204412e-03],\n",
       "        [-1.25834753e-03,  5.76562015e-04, -2.08242889e-03, ...,\n",
       "          1.75443420e-03, -8.83941248e-04, -2.21224967e-03],\n",
       "        [-1.32148014e-03,  5.23037743e-04, -2.37807864e-03, ...,\n",
       "          1.86348602e-03, -6.45818189e-04, -2.38851504e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.58492549e-04, -7.44797580e-05,  1.17044976e-04, ...,\n",
       "          3.48531059e-04, -2.53725913e-04,  4.36148839e-05],\n",
       "        [ 4.56366717e-04,  2.20805941e-05,  3.34703509e-05, ...,\n",
       "          2.38594381e-04, -3.18526610e-04,  1.45487400e-04],\n",
       "        [ 6.40191836e-04, -3.05271271e-04, -4.16892639e-04, ...,\n",
       "          1.45015132e-04, -2.63295165e-04,  3.14528443e-05],\n",
       "        ...,\n",
       "        [-1.59285369e-03, -2.24352465e-04,  4.22505364e-05, ...,\n",
       "          1.23260135e-03, -2.05997203e-04, -9.18973004e-04],\n",
       "        [-1.62305543e-03, -1.63618781e-04, -4.54446068e-04, ...,\n",
       "          1.54857291e-03, -1.73818087e-04, -1.24634698e-03],\n",
       "        [-1.65505230e-03, -1.21405421e-04, -9.46042477e-04, ...,\n",
       "          1.78422988e-03, -1.13259797e-04, -1.53637899e-03]],\n",
       "\n",
       "       [[ 1.58492549e-04, -7.44797580e-05,  1.17044976e-04, ...,\n",
       "          3.48531059e-04, -2.53725913e-04,  4.36148839e-05],\n",
       "        [ 2.20564980e-04,  4.32469213e-04,  3.26266570e-04, ...,\n",
       "          4.01126657e-04, -2.16713815e-04, -1.51112661e-04],\n",
       "        [ 5.71264420e-04,  5.61360270e-04,  7.63892138e-04, ...,\n",
       "          1.89779428e-04, -9.30406823e-05, -3.42460145e-04],\n",
       "        ...,\n",
       "        [-3.19482293e-04,  1.39993173e-03,  1.31403340e-03, ...,\n",
       "          7.67098390e-04, -6.58590579e-04, -6.35368109e-04],\n",
       "        [-4.93572559e-04,  1.43814366e-03,  6.89021079e-04, ...,\n",
       "          1.18747063e-03, -7.03358033e-04, -9.29836067e-04],\n",
       "        [-6.52046176e-04,  1.43715786e-03,  5.94462508e-05, ...,\n",
       "          1.52764330e-03, -6.80721540e-04, -1.20734796e-03]],\n",
       "\n",
       "       [[ 1.58492549e-04, -7.44797580e-05,  1.17044976e-04, ...,\n",
       "          3.48531059e-04, -2.53725913e-04,  4.36148839e-05],\n",
       "        [ 5.96484715e-05, -5.06134274e-05,  1.81173280e-04, ...,\n",
       "          5.44949959e-04, -9.65751824e-05, -2.77073443e-04],\n",
       "        [-2.86519964e-04,  3.41274659e-04,  5.41940040e-04, ...,\n",
       "          7.94883119e-04, -5.16270375e-05, -3.68906651e-04],\n",
       "        ...,\n",
       "        [-7.94744119e-04,  9.40183992e-04,  8.93886609e-04, ...,\n",
       "          5.98171086e-04,  6.14036166e-04, -2.76505505e-03],\n",
       "        [-7.92426290e-04,  9.51418129e-04,  4.15743940e-04, ...,\n",
       "          1.00096851e-03,  4.52449400e-04, -2.84821470e-03],\n",
       "        [-8.27237498e-04,  9.29317262e-04, -1.03779908e-04, ...,\n",
       "          1.35678868e-03,  3.47205700e-04, -2.90922984e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 가져오기 \n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  8704512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  4096      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  4096      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  17426025  \n",
      "=================================================================\n",
      "Total params: 40,826,985\n",
      "Trainable params: 40,822,889\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 정보를 확인 \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "975/975 [==============================] - 220s 223ms/step - loss: 3.2756 - val_loss: 2.8882\n",
      "Epoch 2/10\n",
      "975/975 [==============================] - 261s 267ms/step - loss: 2.7141 - val_loss: 2.6668\n",
      "Epoch 3/10\n",
      "975/975 [==============================] - 260s 267ms/step - loss: 2.4514 - val_loss: 2.5308\n",
      "Epoch 4/10\n",
      "975/975 [==============================] - 261s 268ms/step - loss: 2.2305 - val_loss: 2.4366\n",
      "Epoch 5/10\n",
      "975/975 [==============================] - 261s 267ms/step - loss: 2.0447 - val_loss: 2.3687\n",
      "Epoch 6/10\n",
      "975/975 [==============================] - 261s 267ms/step - loss: 1.8869 - val_loss: 2.3254\n",
      "Epoch 7/10\n",
      "975/975 [==============================] - 261s 267ms/step - loss: 1.7502 - val_loss: 2.2843\n",
      "Epoch 8/10\n",
      "975/975 [==============================] - 261s 268ms/step - loss: 1.6310 - val_loss: 2.2643\n",
      "Epoch 9/10\n",
      "975/975 [==============================] - 262s 268ms/step - loss: 1.5266 - val_loss: 2.2526\n",
      "Epoch 10/10\n",
      "975/975 [==============================] - 262s 268ms/step - loss: 1.4350 - val_loss: 2.2423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feab05e9c40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, validation_data=valid_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "# 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    \n",
    "# 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성\n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "\n",
    "# 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줌\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "#모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    " \n",
    "    generated = \"\"\n",
    "\n",
    "# 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 최종적으로 모델이 생성한 자연어 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> he s a monster <end> \n",
      "<start> she s got me runnin round and round <end> \n",
      "<start> say it all <end> \n",
      "<start> everything i own i give you rocked my world <end> \n",
      "<start> anywhere you wanna <end> \n",
      "<start> somebody s out <end> \n",
      "<start> like a pony would like a pony would <end> \n",
      "<start> why do you love me <end> \n",
      "<start> money , money , money , money <end> \n",
      "<start> love is a losing game <end> \n"
     ]
    }
   ],
   "source": [
    "# text generation 결과 확인 \n",
    "\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> he', max_len=15))\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> she', max_len=15))\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> say', max_len=15))\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> everything', max_len=15))\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> anywhere', max_len=15))\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> somebody', max_len=15))\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> like', max_len=15))\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> why', max_len=15))\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> money', max_len=15))\n",
    "print(generate_text(model, tokenizer, init_sentence='<start> love', max_len=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "####  목표 : val_loss 2.2 수준으로 줄이기, epoch = 10 고정\n",
    "\n",
    "\n",
    "[Epoch 10/10\n",
    "975/975 [==============================] - 262s 268ms/step - loss: 1.4350 - val_loss: 2.2423]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 이번 노드에서는 가사 데이터를 학습 : 특수문자 제거, 토크나이저 생성, 패딩 처리 등의 과정을 거침 \n",
    "- Text Generator 결과는 그럴듯한 문장으로 생성 됨 예) 'why do you love me'\n",
    "- 인공지능이 문장을 이해하는 방식과 작문을 가르치는 법을 배웠는데 Embedding size 와 hidden size 의 기본값만으로 성능은 잘 나와서 데이터의 질이 중요하다는 것을 다시한번 느낄수 있었음\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
